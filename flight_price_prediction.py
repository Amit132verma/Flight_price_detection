# -*- coding: utf-8 -*-
"""Flight_price_prediction (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FQp-o5hxouxleIlnYerlWRUtGEK8mmf
"""

import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""Importing data from data source in this case its a excel file which we downloaded from kaggle"""

dataset=pd.read_csv('flight_data')

dataset.head(3)

"""Here we go our first step ----> Data Preprocessing part"""

dataset.isna().sum() # checking the null values

dataset.dropna(inplace=True) # Droping the null values as it is only a single value

dataset.duplicated().sum() # Checking of the duplicated values

dataset.drop_duplicates(inplace=True) # There are  no benfits of keeping duplicates so drop them

"""So in Date of journey we need to extract day/month/year"""

dataset['day']=pd.to_datetime(dataset['Date_of_Journey']).dt.day
dataset['month']=pd.to_datetime(dataset['Date_of_Journey']).dt.month
dataset['year']=pd.to_datetime(dataset['Date_of_Journey']).dt.year

dataset.drop(columns={'Date_of_Journey'},inplace=True)

dataset.head(3)

################# Preprocessing of the Departure Time Extracting Departure Hour and Minz #######################
dataset['Dep_hr']=dataset['Dep_Time'].str.split(':',expand=True)[0].astype(float)
dataset['Dep_Minz']=dataset['Dep_Time'].str.split(':',expand=True)[1].astype(float)
dataset.drop(columns={'Dep_Time'},inplace=True)

dataset.head(3)

##################################   Extraction of Arrival_Time  ##################################################
val_1=dataset["Arrival_Time"].str.split(":",expand=True)
dataset['Arrival_hr']=val_1[0]
dataset['Arrival_hr']=dataset['Arrival_hr'].astype(float)
val_2=val_1[1].str.split(' ',expand=True)[0]
dataset['Arrival_minz']=val_2[0]
dataset['Arrival_hr']=dataset['Arrival_hr'].astype(float)

dataset.drop(columns={'Arrival_Time'},inplace=True)

dataset.head(3)

################################### Preprocessing the Duration  ##########################################
val_3=dataset['Duration'].str.split(' ',expand=True)
val_3[0].value_counts()
# we encounted a error that a flight duration in hr is 5 min how it is possible so we correct it to 5 hrs
val_3[0]=val_3[0].str.replace('5m','5h')
val_3[0].value_counts()
hours=val_3[0].str.split('h',expand=True)[0]
hours=hours.astype(float)
hours.head(3)

# Prepoocessing the Miniutes Compoment
val_3[1].fillna('0m',inplace=True)
val_3[1].str.split('m',expand=True)[0]
minz=val_3[1].str.split('m',expand=True)[0]
minz=minz.astype(float)
minz=minz/60
minz.head(3)

dataset['Total_Duration_hrs']=hours+minz

dataset['Total_Duration_hrs'].head()

dataset.drop(columns={'Duration'},inplace=True)

dataset.head(3)

dataset['Total_Stops'].unique()

dataset['Total_Stops']=dataset['Total_Stops'].str.replace('non-stop','0')
dataset['Total_Stops']=dataset['Total_Stops'].str.split(" ",expand=True)[0]
dataset['Total_Stops']=dataset['Total_Stops'].astype(float)

dataset.head(3)

dataset=dataset[["Airline","Source","Destination","day","month","year","Total_Stops","Dep_hr","Dep_Minz","Arrival_hr","Arrival_minz","Total_Duration_hrs","Additional_Info","Price"]]

dataset.head(3)

########################################### Lets Move To EDA #######################################################
# How Price Varies with Airlines

# How Price varies with Totla stops

# How Price Varies with Month

# How Price Varies with Departure and arrival Hours

# How Price Varies with Additional Information

# How Duration varies with the price

# Which Source has Maximum number of Flights

# How Number of Flights affects the price from the source

# How do price changes occur with different origins and destinations?

# What is the price difference between Economy and Business class tickets?

dataset['Airline'].value_counts()

# How Price Varies with Airlines
grouped_data=dataset.groupby('Airline')['Price'].mean()
grouped_data

fig=px.bar(grouped_data)
fig.update_layout(xaxis_title='Flights',yaxis_title='Average Price')

# How Price varies with Totla stops
grouped_data_1=dataset.groupby('Total_Stops')['Price'].mean()
grouped_data_1

fig=px.bar(grouped_data_1)
fig.update_layout(xaxis_title='Number of Stops',yaxis_title='Average Price',bargap=0.7)

grouped_data_2_1=dataset.groupby('month')['Price'].mean()
grouped_data_2_1

fig=px.line(grouped_data_2_1)
fig.update_layout(xaxis_title='Month',yaxis_title='Mean_Price')

# Which Source has Maximum number of Flights
grouped_data_3=dataset.groupby('Source')['Airline'].count()
grouped_data_3

fig=px.bar(grouped_data_3)
fig.update_layout(xaxis_title='Source_Name',yaxis_title='Counter')

grouped_data_4=dataset.groupby('Destination')['Airline'].count()
grouped_data_4

fig=px.bar(grouped_data_3)
fig.update_layout(xaxis_title='Destination_Name',yaxis_title='Counter',bargap=0.7)

# # How Price Varies with Departure and arrival Hours
grouped_data_31=dataset.groupby('Dep_hr')['Price'].mean()
grouped_data_31
fig=px.line(grouped_data_31)
fig.update_layout(title='Variation of Price with Departure Hour',yaxis_title='Mean Price')

# # # How Price Varies with Departure and arrival Hours
grouped_data_32=dataset.groupby('Arrival_hr')['Price'].mean()
grouped_data_32
fig=px.line(grouped_data_32)
fig.update_layout(title='Variation of Price with Arrival Hour',yaxis_title='Mean Price')

# # How do price changes occur with different origins and destinations?
grouped_data_33=dataset.groupby('Source')['Price'].mean()
grouped_data_33
fig=px.bar(grouped_data_33)
fig.update_layout(title='Variation of Price with Source',yaxis_title='Mean Price',bargap=0.7)

grouped_data_39=dataset.groupby('Destination')['Price'].mean()
grouped_data_39
fig=px.bar(grouped_data_39)
fig.update_layout(title='Variation of Price with Destination',yaxis_title='Mean Price',bargap=0.7)

# What is the price difference between Economy and Business class tickets?
grouped_data_34=dataset.groupby('Additional_Info')['Price'].mean()
grouped_data_34
fig=px.bar(grouped_data_34)
fig.update_layout(title='Variation of Price with Additional Perks',yaxis_title='Mean Price',bargap=0.7)

################################### Stastical Analysis Of all Aspects #######################################
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import kurtosis
def plot_feature_kurtosis(dataset):
    # Select numeric columns
    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns
    # Set the number of rows and columns for the plot grid
    num_plots = len(numeric_columns)
    num_cols = 3  # Number of columns for subplots
    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows dynamically
    # Set the figure size
    plt.figure(figsize=(15, 5 * num_rows))
    for i, feature in enumerate(numeric_columns, 1):
        plt.subplot(num_rows, num_cols, i)
        # Calculate kurtosis for the feature
        kurtosis_value = kurtosis(dataset[feature].dropna(), fisher=False)  # fisher=False for regular kurtosis
        # Plot distribution using seaborn's distplot
        sns.distplot(dataset[feature].dropna(), kde=True, color='green')
        plt.title(f'{feature} (Kurtosis: {kurtosis_value:.2f})')
    plt.tight_layout()  # Adjust subplots to fit in the figure area
    plt.show()
plot_feature_kurtosis(dataset)

from scipy.stats import skew
def plot_feature_skewness(dataset):
    # Select numeric columns
    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns

    # Set the number of rows and columns for the plot grid
    num_plots = len(numeric_columns)
    num_cols = 3  # Number of columns for subplots
    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows dynamically

    # Set the figure size
    plt.figure(figsize=(15, 5 * num_rows))

    for i, feature in enumerate(numeric_columns, 1):
        plt.subplot(num_rows, num_cols, i)

        # Calculate skewness for the feature
        skewness_value = skew(dataset[feature].dropna())

        # Plot distribution using seaborn's distplot
        sns.distplot(dataset[feature].dropna(), kde=True, color='blue')
        plt.title(f'{feature} (Skewness: {skewness_value:.2f})')

    plt.tight_layout()  # Adjust subplots to fit in the figure area
    plt.show()
plot_feature_skewness(dataset)

#################################### How to Handle Skewness and Kurtosis ###################################
################################## Power Transformer is such a method ######################################
'''
The Box-Cox transformation is a statistical technique used to stabilize variance and make a dataset more
normally distributed. It's particularly useful for skewed data and helps make the distribution of features more
symmetrical,
which can improve the performance of machine learning models.
'''
from sklearn.preprocessing import PowerTransformer

# Sample data (positive values only)
data = np.array([100, 20, 3, 4, 5, 6, 7, 8, 90, 100]).reshape(-1, 1)

# Apply Box-Cox transformation
pt = PowerTransformer(method='box-cox')
transformed_data = pt.fit_transform(data)

# Plot the original and transformed data using Seaborn distplot
plt.figure(figsize=(12, 6))

# Plot original data
plt.subplot(1, 2, 1)
sns.distplot(data, kde=True, color='blue')
plt.title('Original Data')

# Plot transformed data
plt.subplot(1, 2, 2)
sns.distplot(transformed_data, kde=True, color='green')
plt.title('Box-Cox Transformed Data')

plt.tight_layout()
plt.show()

from scipy.stats import skew, boxcox
from scipy.special import inv_boxcox

def plot_feature_skewness(dataset, transformed=False):
    # Select numeric columns
    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns

    # Set the number of rows and columns for the plot grid
    num_plots = len(numeric_columns)
    num_cols = 3  # Number of columns for subplots
    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows dynamically

    # Set the figure size
    plt.figure(figsize=(15, 5 * num_rows))

    for i, feature in enumerate(numeric_columns, 1):
        plt.subplot(num_rows, num_cols, i)

        # Calculate skewness for the feature
        skewness_value = skew(dataset[feature].dropna())

        # Plot distribution using seaborn's distplot
        sns.histplot(dataset[feature].dropna(), kde=True, color='blue')
        plt.title(f'{feature} (Skewness: {skewness_value:.2f})')

    plt.tight_layout()  # Adjust subplots to fit in the figure area
    plt.show()

def apply_boxcox_transform(dataset):
    # Create a copy of the dataset to avoid modifying the original
    transformed_dataset = dataset.copy()

    # Apply Box-Cox transformation to numeric columns
    for feature in transformed_dataset.select_dtypes(include=['float64', 'int64']).columns:
        # Only apply to positive data
        if (transformed_dataset[feature] <= 0).any():
            print(f"Skipping {feature} due to non-positive values.")
            continue
        transformed_dataset[feature], _ = boxcox(transformed_dataset[feature].dropna())

    return transformed_dataset

# Plot skewness before transformation
print("Skewness Before Transformation:")
plot_feature_skewness(dataset)

# Apply Box-Cox transformation
transformed_dataset = apply_boxcox_transform(dataset)

# Plot skewness after transformation
print("Skewness After Transformation:")
plot_feature_skewness(transformed_dataset)

######################################### Handling Outliers #######################################################
# Calculate Q1, Q3, and IQR
q1 = dataset['Price'].quantile(0.25)
q3 = dataset['Price'].quantile(0.75)
IQR = q3 - q1

# Calculate lower and upper bounds
lower_bound = q1 - 1.5 * IQR
upper_bound = q3 + 1.5 * IQR

# Filter out the outliers
filtered_dataset = dataset[(dataset['Price'] >= lower_bound) & (dataset['Price'] <= upper_bound)]

# Create box plot with outliers
fig_with_outliers = px.box(dataset, y='Price', title='Box Plot of Price (With Outliers)')
fig_with_outliers.show()

# Create box plot without outliers
fig_without_outliers = px.box(filtered_dataset, y='Price', title='Box Plot of Price (Without Outliers)')
fig_without_outliers.show()

# KDE plot with outliers
plt.figure(figsize=(10, 6))
sns.kdeplot(data=dataset['Price'], color='blue', fill=True, alpha=0.5)
plt.title('Price Density Plot (With Outliers)')
plt.xlabel('Price')
plt.ylabel('Density')
plt.show()

# KDE plot without outliers
plt.figure(figsize=(10, 6))
sns.kdeplot(data=filtered_dataset['Price'], color='blue', fill=True, alpha=0.5)
plt.title('Price Density Plot (Without Outliers)')
plt.xlabel('Price')
plt.ylabel('Density')
plt.show()

###################################### Best strategy is to Remove Outliers #################################
dataset=filtered_dataset
dataset.shape

dataset.head()

import matplotlib.pyplot as plt
def plot_regression_plots(dataset, variables, target):
    fig, axes = plt.subplots(1, len(variables), figsize=(18, 5))
    for i, var in enumerate(variables):
        sns.regplot(x=dataset[var], y=dataset[target], ax=axes[i], line_kws={'color': 'red'})
        axes[i].set_title(f'{var} vs {target}')
        axes[i].set_xlabel(var)
        axes[i].set_ylabel(target)
    plt.tight_layout()
    plt.show()
plot_regression_plots(dataset, ['Total_Stops', 'Dep_hr', 'Arrival_hr','Total_Duration_hrs','month'], 'Price')

X = dataset.drop(columns=["Price"])
y = dataset["Price"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

categorical_features = ["Airline", "Source", "Destination"]
numerical_features = ["day", "month", "year", "Total_Stops", "Dep_hr", "Dep_Minz", "Arrival_hr", "Arrival_minz", "Total_Duration_hrs"]

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, PowerTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", PowerTransformer(), numerical_features),
        ("cat", OneHotEncoder(), categorical_features)
    ]
)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge

# Define models with hyperparameters to tune
def define_model(trial):
    model_name = trial.suggest_categorical("model", ["Linear Regression", "Ridge", "Random Forest", "XGBoost"])

    if model_name == "Linear Regression":
        model = LinearRegression()

    elif model_name == "Ridge":
        alpha = trial.suggest_loguniform('ridge_alpha', 1e-3, 10.0)
        model = Ridge(alpha=alpha)

    elif model_name == "Random Forest":
        n_estimators = trial.suggest_int("n_estimators", 50, 300)
        max_depth = trial.suggest_int("max_depth", 3, 20)
        min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)

    elif model_name == "XGBoost":
        n_estimators = trial.suggest_int("n_estimators", 50, 300)
        learning_rate = trial.suggest_loguniform("learning_rate", 1e-3, 0.3)
        max_depth = trial.suggest_int("max_depth", 3, 20)
        reg_lambda = trial.suggest_loguniform("reg_lambda", 1e-3, 10.0)
        model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, reg_lambda=reg_lambda, random_state=42)

    return model

# Store training and testing errors
train_errors = []
test_errors = []

from sklearn.metrics import mean_squared_error, r2_score
# Modify the objective function to return both MSE and R²
def objective(trial):
    model = define_model(trial)

    # Define the pipeline
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("model", model)
    ])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Predict on training and testing sets
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    # Calculate MSE for both training and testing sets
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    # Calculate R² for the test set
    test_r2 = r2_score(y_test, y_test_pred)

    # Store errors for plotting later
    train_errors.append(train_mse)
    test_errors.append(test_mse)

    # Return both MSE and R² for multi-objective optimization
    return test_mse, test_r2

import optuna
study = optuna.create_study(directions=["minimize", "maximize"])  # Multi-objective: minimize MSE, maximize R²
study.optimize(objective, n_trials=3350)  # Adjust n_trials as needed more number is Preffered

best_trials = study.best_trials
best_trials

# Initialize variables to track the best model based on specific criteria
best_trial = None
best_mse = float("inf")
best_r2 = float("-inf")
best_model_name = ""

# Iterate over the best trials
for trial in best_trials:
    mse = trial.values[0]  # First value is MSE
    r2 = trial.values[1]   # Second value is R² score

    # Select the trial with the lowest MSE and highest R² score
    if (mse < best_mse) or (mse == best_mse and r2 > best_r2):
        best_mse = mse
        best_r2 = r2
        best_trial = trial
        best_model_name = trial.params["model"]

# Print the best model details
print(f"Best Model: {best_model_name}")
print(f"Best MSE: {best_mse}")
print(f"Best R² Score: {best_r2}")

# Plot training vs testing error
plt.figure(figsize=(10, 6))
plt.plot(range(len(train_errors)), train_errors, label="Training Error", color="blue")
plt.plot(range(len(test_errors)), test_errors, label="Testing Error", color="orange")

pipeline_1 = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", define_model(best_trial))  # Use the best trial to define the model
])

# Fit the pipeline with training data
pipeline_1.fit(X_train, y_train)

# Predict on the test set
y_test_pred = pipeline_1.predict(X_test)

# Predict on the training set
y_train_pred = pipeline_1.predict(X_train)

# Calculate MSE and R² for the training set
train_mse = mean_squared_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

# Calculate MSE and R² for the test set
test_mse_final = mean_squared_error(y_test, y_test_pred)
test_r2_final = r2_score(y_test, y_test_pred)

# Print results
print(f"Training Set MSE: {train_mse:.2f}")
print(f"Training Set R² Score: {train_r2:.2f}")
print(f"Testing Set MSE: {test_mse_final:.2f}")
print(f"Testing Set R² Score: {test_r2_final:.2f}")

# Show some sample predictions
sample_predictions = pd.DataFrame({
    "Actual Price": y_test,
    "Predicted Price": y_test_pred
}).head()

print("\nSample Predictions (Actual vs Predicted Prices):")
print(sample_predictions)

# Plot training vs testing error
plt.figure(figsize=(10, 6))
plt.plot(range(len(train_errors)), train_errors, label="Training Error", color="blue")
plt.plot(range(len(test_errors)), test_errors, label="Testing Error", color="orange")

# New data point for prediction (example data )
new_data = pd.DataFrame({
    "Airline": ["IndiGo"],  # Example airline, change if needed
    "Source": ["Mumbai"],  # Example source, change if needed
    "Destination": ["New Delhi"],  # Example destination, change if needed
    "day": [10],  # Example day
    "month": [11],  # Set month to 8 for the prediction
    "year": [2024],  # Example year
    "Total_Stops": [0],  # Example stop count
    "Dep_hr": [5.0],  # Example departure hour
    "Dep_Minz": [0.0],  # Example departure minutes
    "Arrival_hr": [7.0],  # Example arrival hour
    "Arrival_minz": [0],  # Example arrival minutes
    "Total_Duration_hrs": [2.04],  # Example total duration
    "Additional_Info": ["No info"]  # Additional info
})
# Use the pipeline from the best model to predict
predicted_price = pipeline_1.predict(new_data)
# Output the predicted price
print(f"Predicted Price : {predicted_price[0]:.2f}")

